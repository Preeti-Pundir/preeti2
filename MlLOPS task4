•	Lets create our fully connected layer and our Model
prediction = Dense(len(folders), activation='sigmoid')(x)
model = Model(inputs=vgg.input, outputs=prediction)
model.summary()
model.compile(
  loss= 'binary_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)


The loss function here is binary_crossentropy as our model only have 2 classes.
•	Now I am doing some Augmentation with my dataset of Images for training the model more efficiently.
from keras.preprocessing.image import ImageDataGenerator


train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)


test_datagen = ImageDataGenerator(rescale = 1./255)


training_set = train_datagen.flow_from_directory(train_path,
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'binary')


test_set = test_datagen.flow_from_directory(valid_path,
                                            target_size = (224, 224),
                                            batch_size = 32,
                                            class_mode = 'binary')


r = model.fit_generator(
  training_set,
  validation_data=test_set,
  epochs=7,
  steps_per_epoch=len(training_set),
  validation_steps=len(test_set)
)
•	Its obvious class mode will be binary.
•	Lets plot the loss and accuracies which we will encounter during the epochs..:-
# loss
plt.plot(r.history['loss'], label='train loss')
plt.plot(r.history['val_loss'], label='val loss')
plt.legend()
plt.show()
plt.savefig('LossVal_loss')


# accuracies
plt.plot(r.history['accuracy'], label='train acc')
plt.plot(r.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()
plt.savefig('AccVal_acc')

#saving the model
model.save('C:/Users/lenovo/Desktop/mask_face/mask_detection_model.h5'
After the model is trained it will be saved with your desired name.
•	Also you can tune the hyperparameters accordingly to increase the accuracy rate.
•	To automate the tuning of Hyper-parameters check out my blog on Integration of Machine Learning with DevOps. (click on the hyperlink)
After Training the model You can run it with live webcam feed or any alternative you like for prediction , here is my way of predicting the model by OpenCV live cam feed.
from PIL import Image
from keras.applications.vgg19 import preprocess_input
import base64
from io import BytesIO
import json
import random
import cv2
from keras.models import load_model
import numpy as np
from keras.preprocessing import image
model = load_model('C:/Users/lenovo/Desktop/mask_face/maskmodel.h5')
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')


def face_extractor(img):
    faces=face_cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=5)
    
    if faces is ():
        return None
    for (x,y,w,h) in faces:
        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)
        cropped_face = img[y:y+h, x:x+w]


    return cropped_face
video_capture = cv2.VideoCapture(0)
while True:
    _, frame = video_capture.read()
    face=face_extractor(frame)
    if type(face) is np.ndarray:
        face = cv2.resize(face, (224, 224))
        im = Image.fromarray(face, 'RGB')
        img_array = np.array(im)
        img_array = np.expand_dims(img_array, axis=0)
        pred = model.predict(img_array)
        print(pred)                   
        name="None matching"
        
        if(pred[0][0]==0.):
            name='Mask On'
            cv2.putText(frame,name, (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)
        else:
            name='Mask Not On'
            cv2.putText(frame,name, (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (255,0,0), 2)              
    else:
        cv2.putText(frame,"No face found", (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,155,45), 2)
    cv2.imshow('Video', frame)
    if cv2.waitKey(2)==27:
                break 
   
video_capture.release()
cv2.destroyAllWindows()
•	Now lets do some pre variable creations of Image shapes and file paths of our test and train sets.
IMAGE_SIZE = [224, 224]
train_path = 'C:/Users/lenovo/Desktop/mask_face/Train_set/'
valid_path = 'C:/Users/lenovo/Desktop/mask_face/Test_set/'
folders = glob(train_path)

Assigning this Image Size because VGG16 uses the same image size in its input layer.
•	Then comes the downloading the weights of Our Pre Trained model "VGG16" from the Internet
vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)

I have downloaded the weights of model in the variable "vgg" with the image size as assigned before.
Moving ahead.. as the model VGG16 has its own 16 layers, we have to make them not trainable as in transfer learning we just need experience (weights) of the model to be passed not to train the whole model from Zero level through all those layers...It will become a hectic process and also of no use.

